{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST DNN with Estimators & Layers\n",
    "\n",
    "In this example, we will use a deep neural network to classify the MNIST dataset. This code is very similar to the 5th example (deep_neural_network_mnist_layers), however instead of manually managing the session and TensorBoard, we let Estimators handle this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Estimators, we do not manage the TensorFlow session directly. Instead, we skip straight to defining our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of neurons in each hidden layer\n",
    "HIDDEN1_SIZE = 500\n",
    "HIDDEN2_SIZE = 250\n",
    "\n",
    "NUM_CLASSES = 10  # 10 digits 0-9\n",
    "NUM_PIXELS = 28 * 28  # dataset size.\n",
    "\n",
    "# experiment with the nubmer of training steps to \n",
    "# see the effect\n",
    "TRAIN_STEPS = 2000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we're using a different learning rate than the previous\n",
    "# notebook, and a new optimizer\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our model\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # First we'll create 2 fully-connected layers, with ReLU activations.\n",
    "    fc1 = tf.layers.dense(features, HIDDEN1_SIZE, activation=tf.nn.relu, name=\"fc1\")\n",
    "    fc2 = tf.layers.dense(fc1, HIDDEN2_SIZE, activation=tf.nn.relu, name=\"fc2\")\n",
    "\n",
    "    # Next, we'll apply Dropout to the second layer\n",
    "    # This can help prevent overfitting, and I've added it here\n",
    "    # for illustration. You can comment this out, if you like.\n",
    "    dropped = tf.nn.dropout(fc2, keep_prob=0.9, name=\"dropout1\")\n",
    "\n",
    "    # Finally, we'll calculate logists. This will be\n",
    "    # the input to our Softmax function. Notice we \n",
    "    # don't apply an activation at this layer.\n",
    "    # If you've commented out the dropout layer,\n",
    "    # switch the input here to 'fc2'.\n",
    "    y = tf.layers.dense(dropped, NUM_CLASSES, name=\"output\")\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=labels, logits=y)\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    learning_rate = LEARNING_RATE\n",
    "    # Alternate learning rate calculation\n",
    "    #learning_rate = tf.train.exponential_decay(\n",
    "    #  LEARNING_RATE, tf.train.get_global_step(), 100000, 0.96)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Return an estimator spec that encapsulates your model.\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define our data input function\n",
    "def input_fn():\n",
    "    # Use the new datasets API!\n",
    "    mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "    images = mnist.train.images\n",
    "    labels = mnist.train.labels\n",
    "    assert(images.shape[0] == labels.shape[0])  # Same number of examples\n",
    "    \n",
    "    image_dataset = tf.contrib.data.Dataset.from_tensor_slices(images)\n",
    "    label_dataset = tf.contrib.data.Dataset.from_tensor_slices(labels)\n",
    "    dataset = tf.contrib.data.Dataset.zip((image_dataset, label_dataset))\n",
    "    dataset = dataset.repeat()  # repeat indefinitely\n",
    "    dataset = dataset.shuffle(buffer_size=labels.shape[0])\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the run configuration.\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the estimator using our input function\n",
    "estimator.train(input_fn=input_fn, steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise for the reader: support the other modes for estimators. (i.e. test and predict.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
