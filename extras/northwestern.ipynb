{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello friendly folks at Northwestern! \n",
    "\n",
    "This notebok contains a walkthrough to help you get started with TensorFlow. We'll work on a few exercises for...\n",
    "\n",
    "* Linear Regression, using low-level TensorFlow.  \n",
    "* Logistic Regression, using low-level TensorFlow.  \n",
    "* Deep Neural Network, using low-level TensorFlow.\n",
    "* The above, with Canned Estimators.\n",
    "* Practical stuff: training models on (possibly large) amounts of structured data.\n",
    "* Custom Estimators.\n",
    "\n",
    "Of course, Deep Learning is a wide and rich field, and TensorFlow can do much more than the above. Here are some recent articles you can check out:\n",
    "\n",
    "* https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html\n",
    "* https://research.googleblog.com/2017/07/building-your-own-neural-machine.html\n",
    "* https://magenta.tensorflow.org (for many projects using TensorFlow for art & music).\n",
    "\n",
    "# Installation\n",
    "Before you begin, please make sure you have TensorFlow version 1.3.0rc0 (or higher) installed on your machine, where *\"rc\"* means *\"release candidate\"*. We'll be working with the Datasets API, which we're currently developing to make it easy to efficiently train models on large amounts of data (say, that are too big to fit into memory).\n",
    "\n",
    "\n",
    "# Learning more\n",
    "\n",
    "Here are some short videos and a couple book recommendations.\n",
    "\n",
    "* Machine Learning Recipes: https://goo.gl/uRR7r4\n",
    "* Hands-On Machine Learning with Scikit-Learn and TensorFlow: http://shop.oreilly.com/product/0636920052289.do\n",
    "* Deep Learning with Python: https://www.manning.com/books/deep-learning-with-python\n",
    "\n",
    "You can follow TensorFlow on Twitter, if you like, at https://twitter.com/tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "If you can successfully run this cell, then your machine is properly configured for this tutorial. If the only line that's causing you problems is *import pylab* and the next, you can safely comment those out and skip the cells that use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2 & 3 compatibility\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print('You are running TensorFlow version:', tf.__version__)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Linear Regression (with low-level code)\n",
    "\n",
    "Let's start our journey by taking a look at lower-level TensorFlow code, to get a sense for how things work under the hood. Fear not, intrepid reader - you need not write graph-level code in practice unless you'd like to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate some data\n",
    "\n",
    "This function will create a noisy dataset that's roughly linear, according to the equation *y = mx + b + noise*. As you might expect, we'll then try to find the best fit line. Of course, there's a closed form solution to this - but we'll use gradient descent as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_noisy_data(m=0.1, b=0.3, n=100):\n",
    "    x = np.random.rand(n).astype(np.float32)\n",
    "    noise = np.random.normal(scale=0.01, size=len(x))\n",
    "    y = m * x + b + noise\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = make_noisy_data()\n",
    "x_test, y_test = make_noisy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the following lines to plot the data.\n",
    "# Isn't it beautiful?\n",
    "# Training data is shown in blue, and testing data in green.\n",
    "# pylab.plot(x_train, y_train, 'b.')\n",
    "# pylab.plot(x_test, y_test, 'g.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following line (as you might imagine) clears the graph.\n",
    "# Why do we need it? Jupyter Notebooks maintain state.\n",
    "# If you run this Notebook twice (and forget to reset it), \n",
    "# this line will restore everything to a clean state for you.\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can think of a Session as an 'execution environment' for a graph.\n",
    "# We won't need this until we're ready to run the graph, but\n",
    "# I'll create it now, just for kicks.\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path to a log directory.\n",
    "# As written, it will be created in the same directory as this notebook.\n",
    "# Later, we'll use TensorBoard to visualize data stored \n",
    "# in this directory - and it will be awesome.\n",
    "\n",
    "# Tip:\n",
    "# If you have trouble with TensorBoard, delete\n",
    "# the log directory, and re-run the notebook.\n",
    "LOGDIR = './graphs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define placeholders for data we'll feed to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can think of a 'Placeholder' as a promise. It's a value we \n",
    "# promise to provide when we execute the graph.\n",
    "# A lot of this code is for display purposes.\n",
    "# - 'tf.name_scope' nests our placeholders under an 'input' block\n",
    "# - name='x-input' gives TensorBoard a display name for this node.\n",
    "# shape=[None] means x_placeholder is a one dimensional array of any length. \n",
    "# - this is so we can feed a 'batch' of data later, for example,\n",
    "# - for stochastic gradient descent, or to make predictions.\n",
    "with tf.name_scope('input'):\n",
    "    x_placeholder = tf.placeholder(shape=[None], dtype=tf.float32, name='x-input')\n",
    "    y_placeholder = tf.placeholder(shape=[None], dtype=tf.float32, name='y-input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our model.\n",
    "\n",
    "Here, we'll use a linear model (e.g., *y = mx + b*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('model'):\n",
    "    m = tf.Variable(tf.random_normal([1]), name='m')\n",
    "    b = tf.Variable(tf.random_normal([1]), name='b')\n",
    "    # This is the same as y = tf.add(tf.mul(m, x_placeholder), b), \n",
    "    # but looks nicer\n",
    "    y = m * x_placeholder + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss and Optimizer\n",
    "\n",
    "Define a loss function (*mean squared error*) and an optimizer (*vanilla gradient descent*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.5 # a magic number!\n",
    "# as you gain experience with Deep Learning,\n",
    "# you will become proficient in picking proper\n",
    "# values (or just stop worrying about it)\n",
    "\n",
    "with tf.name_scope('training'):\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.square(y - y_placeholder))\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "        train = optimizer.minimize(loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log values in TensorBoard\n",
    "Later, we'll get this for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the graph\n",
    "writer = tf.summary.FileWriter(LOGDIR)\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "# Attach summaries to Tensors (for TensorBoard visualization)\n",
    "tf.summary.histogram('m', m)\n",
    "tf.summary.histogram('b', b)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "# This op will calculate our summary data when run\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables\n",
    "At this point, our graph is complete - and we're nearly ready to begin training. First, variables must be initialized. Don't forget this line - the fate of the universe is uncertain, if you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notice we're running this line with our session.\n",
    "# All the TensorFlow code prior to this point has\n",
    "# served to define the graph.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'm' and 'b' were initialized to random values\n",
    "# let's see what these were\n",
    "initial_vals = sess.run([m, b])\n",
    "print (\"Initial values for m: %f, b: %f\" % (initial_vals[0], initial_vals[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Here, we'll iteratively update the values for 'm' and 'b' using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS = 201\n",
    "\n",
    "for step in range(TRAIN_STEPS):\n",
    "        \n",
    "    # Session will run two ops:\n",
    "    # - summary_op prepares summary data we'll write to disk in a moment\n",
    "    # - train will use the optimizer to adjust our variables to reduce loss\n",
    "    summary_result, _ = sess.run([summary_op, train], \n",
    "                                  feed_dict={x_placeholder: x_train, \n",
    "                                             y_placeholder: y_train})\n",
    "    # write the summary data to disk\n",
    "    writer.add_summary(summary_result, step)\n",
    "    \n",
    "    # Uncomment the following two lines to watch training happen real time.\n",
    "    if step % 20 == 0:\n",
    "        vals = sess.run([m, b])\n",
    "        print(\"Step: %d, m: %f, b: %f\" % (step, vals[0], vals[1]))\n",
    "    \n",
    "# close the writer when we're finished using it\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the learned values for 'm' and 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If things worked properly, 'm' should be about 0.1, \n",
    "# and 'b' should be about 0.3\n",
    "# (+/- a bit, because we added noise when we generated the data)\n",
    "print (\"Learned values for m: %f, b: %f\" % (sess.run(m), sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the trained model to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to make a prediction!\n",
    "# Remember that x_placeholder must be a vector, hence [2] not just 2.\n",
    "# We expect the result to be (about): 2 * 0.1 + 0.3 + noise ~= 0.5\n",
    "sess.run(y, feed_dict={x_placeholder: [2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start TensorBoard\n",
    "\n",
    "Let's see what we got for all that work logging variables.\n",
    "\n",
    "Start TensorBoard by running this command from a terminal.\n",
    "\n",
    "```$ tensorboard --logdir=graphs```\n",
    "\n",
    "Note: first ```cd``` into the directory that contains this notebook. If you are running TensorFlow in a *virtualenv* and you have opened a new terminal window, be sure to start the *virtualenv* again before running TensorBoard.\n",
    "\n",
    "After you have run this command, open TensorBoard by pointing your browser to *http://localhost:6006* Then, click on the tabs for 'scalars', 'distributions', 'histograms', and 'graphs' to learn more.\n",
    "\n",
    "If you run into trouble, delete LOGDIR (to clear information from previous runs), then re-run this script, and restart TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, using low-level code\n",
    "\n",
    "We will now use a linear model to classify handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MNIST dataset. \n",
    "# It will be downloaded to '/tmp/data' if you don't already have a local copy.\n",
    "mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment these lines to understand the format of the dataset.\n",
    "\n",
    "# 1. There are 55k, 5k, and 10k examples in train, validation, and test.\n",
    "# print ('Train, validation, test: %d, %d, %d' % \n",
    "#       (len(mnist.train.images), len(mnist.validation.images), len(mnist.test.images)))\n",
    "\n",
    "# 2. The format of the labels is 'one-hot'.\n",
    "# The fifth image happens to be a '6'.\n",
    "# This is represented as '[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]'\n",
    "# print (mnist.train.labels[4])\n",
    "\n",
    "# You can find the index of the label, like this:\n",
    "# print (np.argmax(mnist.train.labels[4]))\n",
    "\n",
    "# 3. An image is a 'flattened' array of 28*28 = 784 pixels.\n",
    "# print (len(mnist.train.images[4]))\n",
    "\n",
    "# 4. To display an image, first reshape it to 28x28.\n",
    "# pylab.imshow(mnist.train.images[4].reshape((28,28)), cmap=pylab.cm.gray_r)   \n",
    "# pylab.title('Label: %d' % np.argmax(mnist.train.labels[4])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "NUM_PIXELS = 28 * 28\n",
    "TRAIN_STEPS = 2000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "images = tf.placeholder(dtype=tf.float32, shape=[None, NUM_PIXELS])\n",
    "labels = tf.placeholder(dtype=tf.float32, shape=[None, NUM_CLASSES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "W = tf.Variable(tf.truncated_normal([NUM_PIXELS, NUM_CLASSES]))\n",
    "b = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "y = tf.matmul(images, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=labels))\n",
    "train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables after the model is defined\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for i in range(TRAIN_STEPS):\n",
    "    batch_images, batch_labels = mnist.train.next_batch(BATCH_SIZE)\n",
    "    sess.run(train_step, feed_dict={images: batch_images, labels: batch_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                                  \n",
    "print(\"Accuracy %f\" % sess.run(accuracy, feed_dict={images: mnist.test.images, \n",
    "                                                    labels: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method to make predictions on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(i):\n",
    "    image = mnist.test.images[i]\n",
    "    actual_label = np.argmax(mnist.test.labels[i])\n",
    "    prediction = tf.argmax(y,1)\n",
    "    predicted_label = sess.run(prediction, feed_dict={images: [image]})\n",
    "    print (\"Predicted: %d, actual: %d\" % (predicted, actual))\n",
    "    pylab.imshow(mnist.test.images[i].reshape((28,28)), cmap=pylab.cm.gray_r) \n",
    "    return predicted_label, actual_label\n",
    "\n",
    "predict(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) A Deep Neural Network, using low-level TensorFlow code.\n",
    "\n",
    "Using the magic of automatic differentiation, we will now write a Deep Neural Network to classify handwritten digits. If this seems like a big jump from Linear and Logistic Regression - keep in mind, the goal of this exercise is to show you that the part of the code that does the *\"hard work\"* (training the model) is nearly identical. We need only change the code to specify the model (a stack of fully connected layers instead of y = Wx + b). Once that's done, we can train the DNN using TensorFlow in the *same* way we train the Linear / Logistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of neurons in each hidden layer\n",
    "HIDDEN1_SIZE = 500\n",
    "HIDDEN2_SIZE = 250\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "NUM_PIXELS = 28 * 28\n",
    "\n",
    "# experiment with the nubmer of training steps to \n",
    "# see the effect\n",
    "TRAIN_STEPS = 2000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# we're using a different learning rate than the previous\n",
    "# notebook, and a new optimizer\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "with tf.name_scope('input'):\n",
    "    images = tf.placeholder(tf.float32, [None, NUM_PIXELS], name=\"pixels\")\n",
    "    labels = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to create a fully connected layer\n",
    "def fc_layer(input, size_out, name=\"fc\", activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        size_in = int(input.shape[1])\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"weights\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"bias\")\n",
    "        wx_plus_b = tf.matmul(input, w) + b\n",
    "        if activation: return activation(wx_plus_b)\n",
    "        return wx_plus_b\n",
    "    \n",
    "# The way we initialize variables has an affect on how quickly \n",
    "# training converges. We may explore with different strategies later.\n",
    "# w = tf.Variable(tf.truncated_normal(shape=[size_in, size_out], stddev=1.0 / math.sqrt(float(size_in))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "# First, we'll create two fully connected layers, with ReLU activations\n",
    "fc1 = fc_layer(images, HIDDEN1_SIZE, \"fc1\", activation=tf.nn.relu)\n",
    "fc2 = fc_layer(fc1, HIDDEN2_SIZE, \"fc2\", activation=tf.nn.relu)\n",
    "\n",
    "# Next, we'll apply Dropout to the second layer\n",
    "# This can help prevent overfitting, and I've added it here\n",
    "# for illustration. You can comment this out, if you like.\n",
    "dropped = tf.nn.dropout(fc2, keep_prob=0.9)\n",
    "\n",
    "# Finally, we'll calculate logists. This will be\n",
    "# the input to our Softmax function. Notice we \n",
    "# don't apply an activation at this layer.\n",
    "# If you've commented out the dropout layer,\n",
    "# switch the input here to 'fc2'.\n",
    "y = fc_layer(dropped, NUM_CLASSES, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and an optimizer\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=labels))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # Whereas in the previous notebook we used a vanilla GradientDescentOptimizer\n",
    "    # here, we're using Adam. This is a single line of code change, and more\n",
    "    # importantly, TensorFlow will still automatically analyze our graph\n",
    "    # and determine how to adjust the variables to decrease the loss.\n",
    "    train = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define evaluation\n",
    "with tf.name_scope(\"evaluation\"):\n",
    "    # these there lines are identical to the previous notebook.\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "# We'll use a second FileWriter to summarize accuracy on\n",
    "# the test set. This will let us display it nicely in TensorBoard.\n",
    "train_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"train\"))\n",
    "train_writer.add_graph(sess.graph)\n",
    "test_writer = tf.summary.FileWriter(os.path.join(LOGDIR, \"test\"))\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(TRAIN_STEPS):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "    summary_result, _ = sess.run([summary_op, train], \n",
    "                                    feed_dict={images: batch_xs, labels: batch_ys})\n",
    "\n",
    "    train_writer.add_summary(summary_result, step)\n",
    "    train_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
    "    \n",
    "    # calculate accuracy on the test set, every 100 steps.\n",
    "    # we're using the entire test set here, so this will be a bit slow\n",
    "    if step % 100 == 0:\n",
    "        summary_result, acc = sess.run([summary_op, accuracy], \n",
    "                                       feed_dict={images: mnist.test.images, \n",
    "                                                  labels: mnist.test.labels})\n",
    "        test_writer.add_summary(summary_result, step)\n",
    "        test_writer.add_run_metadata(tf.RunMetadata(), 'step%03d' % step)\n",
    "        print (\"test accuracy: %f at step %d\" % (acc, step))\n",
    "\n",
    "\n",
    "print(\"Accuracy %f\" % sess.run(accuracy, \n",
    "                               feed_dict={images: mnist.test.images,\n",
    "                                          labels: mnist.test.labels}))\n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Linear Regression with a Canned Estimator  \n",
    "\n",
    "Now let's begin working with higher-level code. We will again train a linear regression model, in just a few lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dict = {'x': x_train}\n",
    "train_input = tf.estimator.inputs.numpy_input_fn(x_dict, y_train,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_epochs=None) # repeat forever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe input feature usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [tf.feature_column.numeric_column('x')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model\n",
    "After you run this next block, you should see an output line in the logs resembling:\n",
    "\n",
    "```WARNING:tensorflow:Using temporary folder as model directory: /var/folders/sf/j86k2fg96m96w2hmwlsdrvr8006h_5/T/tmpSkPFHV```\n",
    "\n",
    "You can then start TensorBoard, pointing to that directory like this:\n",
    "\n",
    "```$ tensorboard --logdir=/var/folders/sf/j86k2fg96m96w2hmwlsdrvr8006h_5/T/tmpSkPFHV```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearRegressor(features)\n",
    "estimator.train(train_input, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = tf.estimator.inputs.numpy_input_fn({'x': x_test}, shuffle=False)\n",
    "\n",
    "predictions = list(estimator.predict(data_source))\n",
    "preds = [p['predictions'][0] for p in predictions]\n",
    "\n",
    "#for y in predictions:\n",
    "#    print(y['predictions'])\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.scatter(x_train, y_train)\n",
    "pylab.plot(x_test, np.array(preds), 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this felt easier that the lower-level code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Training Models on (possibly large amounts) of Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "Here, we'll work with the \"Adult dataset\" from the U.S. Census bureau. Our task will be to predict whether an individual makes more than $50,000 a year based attributes such as education, hours of work per week, etc. More about this dataset is [here](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/old.adult.names).\n",
    "\n",
    "### You can adapt this code for a problem you care about\n",
    "\n",
    "Our goal here is to demonstrate how to work with data you might represent in a CSV file. Hopefully, you can adapt this code to a problem you care about. \n",
    "\n",
    "### What if I have *lots* of data?\n",
    "\n",
    "The code presented here can be adapted to any CSV dataset that fits in memory (using the *pandas input function*) or a dataset of pretty much any size (using the *Datasets API*, below) - which contains logic to efficiently read it from disk. When you're training large models using GPUs, you want to be sure your input pipeline doesn't bottleneck (or starve) the GPU. The Datasets API handle this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "census_train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "census_train_path = tf.contrib.keras.utils.get_file('census.train', census_train_url)\n",
    "\n",
    "census_test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "census_test_path = tf.contrib.keras.utils.get_file('census.test', census_test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = [\n",
    "  'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "  'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "  'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "  'income'\n",
    "]\n",
    "\n",
    "census_train = pd.read_csv(census_train_path, index_col=False, names=column_names) \n",
    "census_test = pd.read_csv(census_train_path, index_col=False, names=column_names) \n",
    "\n",
    "census_train_label = census_train.pop('income') == \" >50K\" \n",
    "census_test_label = census_test.pop('income') == \" >50K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "census_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_train_label[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = tf.estimator.inputs.pandas_input_fn(\n",
    "    census_train, \n",
    "    census_train_label,\n",
    "    shuffle=True, \n",
    "    batch_size = 32, # process 32 examples at a time\n",
    "    num_epochs=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = tf.estimator.inputs.pandas_input_fn(\n",
    "    census_test, \n",
    "    census_test_label, \n",
    "    shuffle=True, \n",
    "    num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = train_input()\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    tf.feature_column.numeric_column('hours-per-week'),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('education-num'), list(range(25))),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('sex', ['male','female']),\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearClassifier(features, model_dir='census/linear',n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(train_input, steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [p for p in estimator.predict(test_input)]\n",
    "print (predictions[0][\"probabilities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update input pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    tf.feature_column.numeric_column('education-num'),\n",
    "    tf.feature_column.numeric_column('hours-per-week'),\n",
    "    tf.feature_column.numeric_column('age'),\n",
    "    tf.feature_column.indicator_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list('sex',['male','female'])),\n",
    "    tf.feature_column.embedding_column(  # now using embedding!\n",
    "        tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000), 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.DNNClassifier(hidden_units=[20,20], \n",
    "                                       feature_columns=features, \n",
    "                                       n_classes=2, \n",
    "                                       model_dir='census/dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(train_input, steps=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.evaluate(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Input Pipeline using Datasets API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def census_input_fn(path):\n",
    "    def input_fn():    \n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(path)\n",
    "                .map(csv_decoder)\n",
    "                .shuffle(buffer_size=100)\n",
    "                .batch(32)\n",
    "                .repeat())\n",
    "\n",
    "        columns = dataset.make_one_shot_iterator().get_next()\n",
    "        income = tf.equal(columns.pop('income'),\" >50K\") \n",
    "        return columns, income\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_defaults = collections.OrderedDict([\n",
    "  ('age',[0]),\n",
    "  ('workclass',['']),\n",
    "  ('fnlwgt',[0]),\n",
    "  ('education',['']),\n",
    "  ('education-num',[0]),\n",
    "  ('marital-status',['']),\n",
    "  ('occupation',['']),\n",
    "  ('relationship',['']),\n",
    "  ('race',['']),\n",
    "  ('sex',['']),\n",
    "  ('capital-gain',[0]),\n",
    "  ('capital-loss',[0]),\n",
    "  ('hours-per-week',[0]),\n",
    "  ('native-country',['']),\n",
    "  ('income',['']),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_decoder(line):\n",
    "  parsed = tf.decode_csv(line, csv_defaults.values())\n",
    "  return dict(zip(csv_defaults.keys(), parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "census_input = census_input_fn(census_train_path)\n",
    "training_batch = census_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    features, high_income = sess.run(training_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features['education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(high_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) A Custom Estimator for a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,test = tf.contrib.keras.datasets.mnist.load_data()\n",
    "x_train,y_train = train \n",
    "x_test,y_test = test\n",
    "\n",
    "mnist_train_input = tf.estimator.inputs.numpy_input_fn({'x':np.array(x_train, dtype=np.float32)},\n",
    "                                                       np.array(y_train,dtype=np.int32),\n",
    "                                                       shuffle=True,\n",
    "                                                       num_epochs=None)\n",
    "\n",
    "mnist_test_input = tf.estimator.inputs.numpy_input_fn({'x':np.array(x_test, dtype=np.float32)},\n",
    "                                                      np.array(y_test,dtype=np.int32),\n",
    "                                                      shuffle=True,\n",
    "                                                      num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### tf.estimator.LinearClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearClassifier([tf.feature_column.numeric_column('x',shape=784)], \n",
    "                                          n_classes=10,\n",
    "                                          model_dir=\"mnist/linear\")\n",
    "estimator.train(mnist_train_input, steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(mnist_test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the results with [TensorBoard](http://0.0.0.0:6006)\n",
    "$> tensorboard --logdir mnnist/DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.DNNClassifier(hidden_units=[256],\n",
    "                                       feature_columns=[tf.feature_column.numeric_column('x',shape=784)], \n",
    "                                       n_classes=10,\n",
    "                                       model_dir=\"mnist/DNN\")\n",
    "estimator.train(mnist_train_input, steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(mnist_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 128\n",
    "STEPS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_layer, mode):\n",
    "    with tf.name_scope(\"conv1\"):  \n",
    "      conv1 = tf.layers.conv2d(inputs=input_layer,filters=32, kernel_size=[5, 5],\n",
    "                               padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    with tf.name_scope(\"pool1\"):  \n",
    "      pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.name_scope(\"conv2\"):  \n",
    "      conv2 = tf.layers.conv2d(inputs=pool1,filters=64, kernel_size=[5, 5],\n",
    "                               padding='same', activation=tf.nn.relu)\n",
    "\n",
    "    with tf.name_scope(\"pool2\"):  \n",
    "      pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    with tf.name_scope(\"dense\"):  \n",
    "      pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "      dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    with tf.name_scope(\"dropout\"):  \n",
    "      is_training_mode = mode == tf.estimator.ModeKeys.TRAIN\n",
    "      dropout = tf.layers.dropout(inputs=dense, rate=0.4, training=is_training_mode)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "  # Describing the model\n",
    "  input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\n",
    "    \n",
    "  tf.summary.image('mnist_input',input_layer)\n",
    "    \n",
    "  logits = build_cnn(input_layer, mode)\n",
    " \n",
    "  # Generate Predictions\n",
    "  classes = tf.argmax(input=logits, axis=1)\n",
    "  predictions = {\n",
    "      'classes': classes,\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Return an EstimatorSpec object\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "  with tf.name_scope('loss'):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "  \n",
    "  loss = tf.reduce_sum(loss)\n",
    "  tf.summary.scalar('loss', loss)\n",
    "    \n",
    "  with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.cast(tf.equal(tf.cast(classes,tf.int32),labels),tf.float32)\n",
    "  accuracy = tf.reduce_mean(accuracy)\n",
    "  tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "  # Configure the Training Op (for TRAIN mode)\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step(),\n",
    "        learning_rate=1e-4,\n",
    "        optimizer='Adam')\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,\n",
    "                                      loss=loss, train_op=train_op)\n",
    "\n",
    "  # Configure the accuracy metric for evaluation\n",
    "  eval_metric_ops = {\n",
    "      'accuracy': tf.metrics.accuracy(\n",
    "          classes,\n",
    "          input=labels)\n",
    "  }\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions,\n",
    "                                    loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create estimator\n",
    "run_config = tf.contrib.learn.RunConfig(model_dir='mnist/CNN')\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)\n",
    "\n",
    "# train for 1000 steps\n",
    "# this is too few\n",
    "estimator.train(input_fn=mnist_train_input, steps=1000)\n",
    "\n",
    "# evaluate\n",
    "estimator.evaluate(input_fn=mnist_test_input)\n",
    "\n",
    "# predict\n",
    "preds = estimator.predict(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed tensorflow: using experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run an experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "# Enable TensorFlow logs\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create experiment\n",
    "def experiment_fn(run_config, hparams):\n",
    "  # create estimator\n",
    "  estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                     config=run_config)\n",
    "  return tf.contrib.learn.Experiment(\n",
    "      estimator,\n",
    "      train_input_fn=train_input_fn,\n",
    "      eval_input_fn=test_input_fn,\n",
    "      train_steps=STEPS\n",
    "  )\n",
    "\n",
    "# run experiment\n",
    "learn_runner.run(experiment_fn,\n",
    "    run_config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the results with [TensorBoard](http://0.0.0.0:6006)\n",
    "$> tensorboard --logdir mnist/CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
